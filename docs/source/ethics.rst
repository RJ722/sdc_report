Moral Ethics
============

With the emergence of automated automobiles, various ethical and moral dilemmas
arise.

While the introduction of automated vehicles to the mass market is said to be
inevitable due to a (presumed but untestable) potential for reduction of crashes
by "up to" 90% and their potential greater accessibility to disabled,
elderly, and young passengers, a range of ethical issues have not been fully
addressed. Those include, but are not limited to: the moral, financial, and
criminal responsibility for crashes and breaches of law. The most prominent of
all being:

.. rubric:: Privacy issues including potential for mass surveillance

Privacy-related issues arise mainly from the interconnectivity of automated
cars, making it just another mobile device that can gather any  information abou
an individual. This information gathering ranges from tracking 
of the routes taken, voice recording, video recording, preferences  in media t
atis consumed in the car, behavioral patterns, to many m ore streams of
information. The data and communications infrastructure n eeded to support the
evehicles may also be capable of surveillance, especial ly if coupled to other
data sets and advanced analytics.

.. rubric:: Unemployment - potential for massive job losses and de-skilling
 
The implementation of automated vehicles to the mass market might cost up to 5
million jobs in the US alone, making up almost 3% of the workforce. Those jobs
include drivers of taxis, buses, vans, trucks, and e-hailing vehicles. Many
industries, such as the auto insurance industry are indirectly affected. This
industry alone generates an annual revenue of about $220 billion, supporting
277,000 jobs. To put this into perspective – this is about the number of
mechanical engineering jobs. The potential loss of a majority of those jobs will
have a tremendous impact on those individuals involved. Both India and China
have placed bans on automated cars with the former citing protection of jobs. 

Other issues include:

- Exposure to hacking and malware.

- Further concentration of market and data power in the hands of a few global
  conglomerates capable of consolidating AI capacity, and of lobbying
  governments to facilitate the shift of liability onto others and their
  potential destruction of existing occupations and industries.

There are different opinions on who should be held liable in case of a crash,
especially with people being hurt. Many experts see the car manufacturers
themselves responsible for those crashes that occur due to a technical
malfunction or misconstruction. Besides the fact that the car manufacturer would
be the source of the problem in a situation where a car crashes due to a
technical issue, there is another important reason why car manufacturers could
be held responsible: *it would encourage them to innovate and heavily invest
into fixing those issues*, not only due to the protection of the brand image,
but also due to financial and criminal consequences.

However, there are also voices that argue those using or owning the vehicle
should be held responsible since they know the risks involved in using such a
vehicle. Experts suggest introducing a tax or insurance that would protect
owners and users of automated vehicles of claims made by victims of an accident.


Other possible parties that can be held responsible in case of a technical
failure include software engineers that programmed the code for the automated
operation of the vehicles, and suppliers of components of the AV. 

The Trolley Problem
*******************

Taking aside the question of legal liability and moral responsibility, the
question arises how automated vehicles should be programmed to behave in an
emergency situation where either passengers or other traffic participants like:
pedestrians, bicyclists and other drivers are endangered. A moral dilemma that a
software engineer or car manufacturer might face in programming the operating
software is described in an ethical thought experiment, the trolley problem: a
conductor of a trolley has the choice of staying on the planned track and
running over five people, or turn the trolley onto a track where it would kill
only one person, assuming there is no traffic on it. [21] When a self-driving
car is in following scenario: it’s driving with passengers and suddenly a person
appears in its way. The car has to decide between the two options, either to run
the person over or to avoid hitting the person by swerving into a wall, killing
the passengers. There are two main considerations that need to be addressed.
First, what moral basis would be used by an automated vehicle to make decisions?
Second, how could those be translated into software code? Researchers have
suggested, in particular, two ethical theories to be applicable to the behavior
of automated vehicles in cases of emergency: Asimov’s [[Three Laws of
Robotics—three laws of robotics]] are a typical example of deontological ethics.
The theory suggests that an automated car needs to follow strict written-out
rules that it needs to follow in any situation. Utilitarianism suggests the idea
that any decision must be made based on the goal to maximize utility. This needs
a definition of utility which could be maximizing the number of people surviving
in a crash. Critics suggest that automated vehicles should adapt a mix of
multiple theories to be able to respond morally right in the instance of a
crash.[20]

The first accidental death from a self-driving-car crash
********************************************************

An incident that happened recently was Uber's self-driving car that struck and
killed a pedestrian in March 2018 had serious software flaws, including the
inability to recognize jaywalkers, according to the NTSB. The US safety agency
said that Uber's software failed to recognize the 49-year-old victim, Elaine
Herzberg, as a pedestrian crossing the street. It didn't calculate that it could
potentially collide with her until 1.2 seconds before impact, at which point it
was too late to brake. More surprisingly, the NTSB said Uber's system design
"did not include a consideration for jaywalking pedestrians." On top of that,
the car initiated a one second braking delay so that the vehicle could calculate
an alternative path or let the safety driver take control. (Uber has since
eliminated that function in a software update.)

    Although the [system] detected the pedestrian nearly six seconds before
    impact, it never classified her as a pedestrian, because she was crossing at
    a location without a crosswalk [and] the system design did not include a
    consideration for jaywalking pedestrians.

Many 'Trolley' discussions skip over the practical problems of how a
probabilistic machine learning vehicle AI could be sophisticated enough to
understand that a deep problem of moral philosophy is presenting itself from
instant to instant while using a dynamic projection into the near future, what
sort of moral problem it actually would be if any, what the relevant weightings
in human value terms should be given to all the other humans involved who will
be probably unreliably identified, and how reliably it can assess the probable
outcomes. These practical difficulties, and those around testing and assessment
of solutions to them, may present as much of a challenge as the theoretical
abstractions. 
